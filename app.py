import requests
from bs4 import BeautifulSoup
import json
import os
from datetime import datetime

def scrape_oliveyoung_rankings():
    api_key = os.getenv("SCRAPER_API_KEY")
    if not api_key:
        raise ValueError("SCRAPER_API_KEY is not set in GitHub Secrets.")

    # ì´ì œ ë°ì´í„° APIê°€ ì•„ë‹Œ, ì‚¬ëŒì´ ë³´ëŠ” ì‹¤ì œ ë­í‚¹ í˜ì´ì§€ ì£¼ì†Œë¥¼ ëª©í‘œë¡œ í•©ë‹ˆë‹¤.
    target_url = "https://www.oliveyoung.co.kr/store/ranking/getBest.do"

    # 'ë§ŒëŠ¥ ì—´ì‡ '(&render=true)ë¥¼ ì‚¬ìš©í•˜ì—¬, ScraperAPIê°€ JSë¥¼ ëª¨ë‘ ì‹¤í–‰í•˜ê³  ìµœì¢… HTMLì„ ê°€ì ¸ì˜¤ë„ë¡ í•©ë‹ˆë‹¤.
    scraperapi_url = f'http://api.scraperapi.com?api_key={api_key}&url={target_url}&render=true'

    print("Sending request via ScraperAPI with Browser Rendering enabled...")
    response = requests.get(scraperapi_url, timeout=180) # ë Œë”ë§ì„ ìœ„í•´ íƒ€ì„ì•„ì›ƒì„ 3ë¶„ìœ¼ë¡œ ë„‰ë„‰í•˜ê²Œ ì„¤ì •

    if response.status_code != 200:
        print(f"âŒ ScraperAPI failed with status code: {response.status_code}")
        print(response.text)
        return None

    try:
        # ì´ì œ JSONì´ ì•„ë‹Œ, ìµœì¢… ê²°ê³¼ë¬¼ì¸ HTMLì„ ë¶„ì„í•©ë‹ˆë‹¤.
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # ë­í‚¹ ë¦¬ìŠ¤íŠ¸ì˜ ê° ì•„ì´í…œì„ ì„ íƒí•©ë‹ˆë‹¤.
        product_list = soup.select('#rank-best-list .prd_item')
        
        if not product_list:
            raise ValueError("Could not find the product list. The page structure might have changed.")

        top_products = []
        for item in product_list[:100]: # 100ìœ„ê¹Œì§€ë§Œ ê°€ì ¸ì˜µë‹ˆë‹¤.
            rank_element = item.select_one('.prd_rank')
            brand_element = item.select_one('.prd_brand')
            name_element = item.select_one('.prd_name')

            if rank_element and brand_element and name_element:
                rank = rank_element.text.strip()
                brand = brand_element.text.strip()
                name = name_element.text.strip()
                top_products.append(f"{rank}. [{brand}] {name}")
            
        return top_products

    except Exception as e:
        print(f"âŒ An error occurred during parsing: {e}")
        print("Response from server was (first 500 chars):")
        print(response.text[:500])
        return None

def send_to_slack(message_lines, is_error=False):
    webhook_url = os.getenv("SLACK_WEBHOOK_URL")
    if not webhook_url: return

    text = f"ğŸš¨ ì˜¬ë¦¬ë¸Œì˜ ë­í‚¹ ìˆ˜ì§‘ ì‹¤íŒ¨" if is_error else f"ğŸ† ì˜¬ë¦¬ë¸Œì˜ ë­í‚¹ Top {len(message_lines[:10])}"
    
    blocks = [
        {"type": "section", "text": {"type": "mrkdwn", "text": f"*{text}*"}},
        {"type": "divider"},
    ]
    if not is_error and message_lines:
        blocks.append({"type": "section", "text": {"type": "mrkdwn", "text": "\n".join(message_lines[:10])}})
    elif is_error and message_lines:
         blocks.append({"type": "section", "text": {"type": "mrkdwn", "text": message_lines[0]}})

    blocks.append({"type": "context", "elements": [{"type": "mrkdwn", "text": f"ğŸ•’ {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}"}]})

    try:
        requests.post(webhook_url, json={"text": text, "blocks": blocks}, timeout=10).raise_for_status()
        print("âœ… Slack message sent successfully")
    except Exception as e:
        print(f"âŒ Failed to send Slack message: {e}")

if __name__ == "__main__":
    print("ğŸ” ì˜¬ë¦¬ë¸Œì˜ ë­í‚¹ ìˆ˜ì§‘ ì‹œì‘ (ScraperAPI + HTML Parsing ìµœì¢… ëª¨ë“œ)")
    rankings = scrape_oliveyoung_rankings()

    if rankings:
        print(f"âœ… Successfully scraped {len(rankings)} items.")
        send_to_slack(rankings)
    else:
        print("âŒ Scraping failed.")
        send_to_slack(["ScraperAPIë¥¼ í†µí•œ ìš”ì²­ ë˜ëŠ” HTML ë¶„ì„ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤."], is_error=True)
