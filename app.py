import os
import re
import base64
import json
from datetime import datetime, timedelta
import pandas as pd
from playwright.sync_api import sync_playwright
from bs4 import BeautifulSoup
from googleapiclient.discovery import build
from googleapiclient.http import MediaFileUpload
from google.oauth2.service_account import Credentials
import requests

# =========================
# í™˜ê²½ ë³€ìˆ˜
# =========================
SLACK_WEBHOOK_URL = os.getenv("SLACK_WEBHOOK_URL")
GDRIVE_SA_JSON_B64 = os.getenv("GDRIVE_SA_JSON_B64")
GDRIVE_FOLDER_ID = os.getenv("GDRIVE_FOLDER_ID")

# =========================
# í´ë” ìƒì„±
# =========================
BASE_DIR = "rankings"
TODAY = datetime.now().strftime("%Y-%m-%d")
os.makedirs(BASE_DIR, exist_ok=True)
os.makedirs(os.path.join(BASE_DIR, TODAY), exist_ok=True)
CSV_PATH = os.path.join(BASE_DIR, TODAY, f"oliveyoung_{TODAY}.csv")

# =========================
# êµ¬ê¸€ ë“œë¼ì´ë¸Œ ì—…ë¡œë“œ
# =========================
def upload_to_drive(local_path, folder_id):
    creds_json = base64.b64decode(GDRIVE_SA_JSON_B64).decode()
    creds_dict = json.loads(creds_json)
    creds = Credentials.from_service_account_info(
        creds_dict, scopes=['https://www.googleapis.com/auth/drive.file']
    )
    service = build('drive', 'v3', credentials=creds)
    file_metadata = {"name": os.path.basename(local_path), "parents": [folder_id]}
    media = MediaFileUpload(local_path, mimetype="text/csv")
    service.files().create(body=file_metadata, media_body=media, fields="id").execute()
    print(f"âœ… Google Drive ì—…ë¡œë“œ ì™„ë£Œ: {os.path.basename(local_path)}")

# =========================
# ì œí’ˆëª… ì „ì²˜ë¦¬
# =========================
def clean_product_name(name):
    return re.sub(r"^\[.*?\]\s*", "", name).strip()

# =========================
# ì˜¬ë¦¬ë¸Œì˜ ë­í‚¹ í¬ë¡¤ë§
# =========================
def scrape_oliveyoung():
    url = "https://www.oliveyoung.co.kr/store/main/getBestList.do"
    print("ğŸ” ì˜¬ë¦¬ë¸Œì˜ ë­í‚¹ ìˆ˜ì§‘ ì‹œì‘")

    with sync_playwright() as p:
        browser = p.chromium.launch(headless=True)
        page = browser.new_page()
        page.goto(url, timeout=60000)
        page.wait_for_timeout(5000)
        html = page.content()
        browser.close()

    soup = BeautifulSoup(html, "html.parser")
    items = soup.select(".prd_info")

    data = []
    rank_counter = 1
    for item in items:
        rank_tag = item.select_one(".num")
        rank_text = rank_tag.get_text(strip=True) if rank_tag else ""
        if "ì˜¤íŠ¹" in rank_text or not rank_text.isdigit():
            rank_text = "[ì˜¤íŠ¹]"
        else:
            rank_text = str(rank_counter)

        brand = item.select_one(".tx_brand").get_text(strip=True) if item.select_one(".tx_brand") else ""
        product = item.select_one(".tx_name").get_text(strip=True) if item.select_one(".tx_name") else ""
        price = item.select_one(".tx_cur").get_text(strip=True) if item.select_one(".tx_cur") else ""

        data.append({"ìˆœìœ„": rank_text, "ë¸Œëœë“œ": brand, "ì œí’ˆëª…": product, "ê°€ê²©": price})
        rank_counter += 1

    df = pd.DataFrame(data)
    df.to_csv(CSV_PATH, index=False, encoding="utf-8-sig")
    print(f"âœ… CSV ì €ì¥ ì™„ë£Œ: {CSV_PATH}")
    return df

# =========================
# ë­í‚¹ ë¶„ì„
# =========================
def analyze_trends(df_today):
    yesterday_str = (datetime.now() - timedelta(days=1)).strftime("%Y-%m-%d")
    yesterday_path = os.path.join(BASE_DIR, yesterday_str, f"oliveyoung_{yesterday_str}.csv")
    if not os.path.exists(yesterday_path):
        print("âš  ì–´ì œ ë°ì´í„° ì—†ìŒ, íŠ¸ë Œë“œ ë¶„ì„ ë¶ˆê°€")
        return ""

    df_yesterday = pd.read_csv(yesterday_path)
    trends = "ğŸ”¥ ê¸‰ìƒìŠ¹ ë¸Œëœë“œ\n"

    # ê¸‰ìƒìŠ¹
    for _, row in df_today.iterrows():
        today_rank = row["ìˆœìœ„"]
        if today_rank == "[ì˜¤íŠ¹]":
            continue
        today_rank = int(today_rank)

        match = df_yesterday[df_yesterday["ì œí’ˆëª…"] == row["ì œí’ˆëª…"]]
        if not match.empty:
            y_rank = match.iloc[0]["ìˆœìœ„"]
            if y_rank != "[ì˜¤íŠ¹]" and str(y_rank).isdigit():
                y_rank = int(y_rank)
                diff = y_rank - today_rank
                if diff >= 10:
                    trends += f"- {row['ë¸Œëœë“œ']}: {y_rank}ìœ„ â†’ {today_rank}ìœ„ (+{diff})\n â–¶ {clean_product_name(row['ì œí’ˆëª…'])}\n"
        else:
            if today_rank <= 10:
                trends += f"- {row['ë¸Œëœë“œ']}: ì²« ë“±ì¥ {today_rank}ìœ„\n â–¶ {clean_product_name(row['ì œí’ˆëª…'])}\n"

    # ê¸‰í•˜ë½
    trends += "\nğŸ“‰ ê¸‰í•˜ë½ ë¸Œëœë“œ\n"
    for _, row in df_yesterday.iterrows():
        y_rank = row["ìˆœìœ„"]
        if y_rank == "[ì˜¤íŠ¹]" or not str(y_rank).isdigit():
            continue
        y_rank = int(y_rank)
        match = df_today[df_today["ì œí’ˆëª…"] == row["ì œí’ˆëª…"]]
        if y_rank <= 50 and (match.empty or (match.iloc[0]["ìˆœìœ„"] != "[ì˜¤íŠ¹]" and int(match.iloc[0]["ìˆœìœ„"]) > 50)):
            trends += f"- {row['ë¸Œëœë“œ']}: {y_rank}ìœ„ â†’ ë­í¬ì•„ì›ƒ\n â–¶ {clean_product_name(row['ì œí’ˆëª…'])}\n"

    return trends

# =========================
# Slack ì „ì†¡
# =========================
def send_to_slack(message):
    if not SLACK_WEBHOOK_URL:
        print("âš  SLACK_WEBHOOK_URL ë¯¸ì„¤ì •")
        return
    res = requests.post(SLACK_WEBHOOK_URL, json={"text": message})
    print("âœ… Slack ì „ì†¡ ì™„ë£Œ" if res.status_code == 200 else f"âŒ Slack ì „ì†¡ ì‹¤íŒ¨: {res.status_code}")

# =========================
# ë©”ì¸ ì‹¤í–‰
# =========================
df_today = scrape_oliveyoung()
upload_to_drive(CSV_PATH, GDRIVE_FOLDER_ID)

# Top 10 ì¶œë ¥
top10_msg = f":bar_chart: ì˜¬ë¦¬ë¸Œì˜ ì „ì²´ ë­í‚¹ (êµ­ë‚´) ({TODAY})\n"
for _, row in df_today.head(10).iterrows():
    top10_msg += f"{row['ìˆœìœ„']}. {row['ë¸Œëœë“œ']} {row['ì œí’ˆëª…']} â€” {row['ê°€ê²©']}\n"

trend_msg = analyze_trends(df_today)
final_msg = f"{top10_msg}\n\n{trend_msg}"

send_to_slack(final_msg)
